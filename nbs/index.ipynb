{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cancer_proj.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cancer-proj\n",
    "\n",
    "> Cancer BT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package implementing results for masters thesis at UNSW and upcoming paper. Comparison of supervised transfer learning vs. self-supervised transfer learning on cancer image dataset (ISIC) with limited labelled data.\n",
    "\n",
    "Keywords: Barlow Twins, 1cycle policy, transfer learning, cancer image classification, skin cancer, low labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install git+https://github.com/hamish-haggerty/cancer-proj.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Full documentation and code examples are upcoming. For now easiest way to use code is to open `cancer_results` from experiments folder in Colab. You will need to download the dataset and save it to your google drive to use that notebook code as is. \n",
    "\n",
    "Transfer learning can be done by a single call to `main_tune` function, once several variables have been defined, including the initial weights, dataloaders, augmentation pipelines, etc. This is all clear in `cancer_results` notebook.\n",
    "\n",
    "In `experiments/bt_cancer_results` we also implement what we call `pre-pretraining.` This means we pretrain with Barlow Twins on the target data, using an already pretrained network (on ImageNet, with Barlow Twins) as the starting weights. This network is then fine tuned. The `pre-pretraining` involves reinitialising a projector network, and freezing the pretrained encoder while the projector only is updated for several epochs. In other words, we use transfer learning but in a self-supervised manner. The code can be used as a black box without understanding these details however (or, see upcoming paper).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
