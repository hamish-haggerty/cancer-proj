{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cancer_maintrain\n",
    "\n",
    "> Basic extensible API to train encoder and fine tune. Written in such a way that we can easily patch in modifications as needed. We perform validation / experiments elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cancer_maintrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Surely there is a way to get rid of having to put this cell everywhere. hmmm.\n",
    "\n",
    "Or we can just copy paste / delete this in and out when needed. Either way, getting close to a decent workable workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def colab_is_true():\n",
    "\n",
    "    try: \n",
    "        from google.colab import drive\n",
    "\n",
    "        return True \n",
    "    except ModuleNotFoundError:\n",
    "        return False\n",
    "\n",
    "def setup_colab():\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    #os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"')\n",
    "    os.system('git clone https://github.com/hamish-haggerty/cancer-proj.git')\n",
    "    os.chdir('cancer-proj')\n",
    "    os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"') #does this work?\n",
    "    os.system('pip install -e .')\n",
    "    os.system('pip install -qU nbdev')\n",
    "    os.system('nbdev_install_quarto')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    on_colab = colab_is_true()\n",
    "    if on_colab:\n",
    "        setup_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "from base_rbt.all import *\n",
    "from cancer_proj.cancer_dataloading import *\n",
    "from cancer_proj.cancer_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Since we have cloned repository and cd'd into it (and the data itself is not stored in the\n",
    "#repo) we need cd out of it, get the data, then cd back into the repo `cancer-proj`.\n",
    "#This is a bit annoying, can maybe remove this later\n",
    "if on_colab:\n",
    "    #os.chdir('..') #assumes we are currently in cancer-proj directory\n",
    "    train_dir = colab_train_dir\n",
    "    test_dir = colab_test_dir\n",
    "else:\n",
    "    train_dir = local_train_dir\n",
    "    test_dir = local_test_dir\n",
    "\n",
    "#define general hps\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#bs=256\n",
    "#bs=698\n",
    "bs=256\n",
    "bs_tune=256\n",
    "size=128\n",
    "bs_val=174\n",
    "\n",
    "#get the data dictionary\n",
    "data_dict = get_fnames_dls_dict(train_dir=train_dir,test_dir=test_dir,\n",
    "                    device=device,bs_val=bs_val,bs=bs,bs_tune=bs_tune,size=size,n_in=3)\n",
    "\n",
    "#get the dataloaders\n",
    "dls_train,dls_tune,dls_valid = data_dict['dls_train'],data_dict['dls_tune'],data_dict['dls_valid']\n",
    "x,y = data_dict['x'],data_dict['y']\n",
    "xval,yval = data_dict['xval'],data_dict['yval']\n",
    "xtune,ytune = data_dict['xtune'],data_dict['ytune']\n",
    "vocab = data_dict['vocab']\n",
    "\n",
    "#If we want to write some tests (make sure the data is same every time etc):\n",
    "fnames,fnames_train,fnames_tune,fnames_valid,fnames_test = data_dict['fnames'],data_dict['fnames_train'],data_dict['fnames_tune'],data_dict['fnames_valid'],data_dict['fnames_test']\n",
    "\n",
    "test_eq(x.shape,xtune.shape)\n",
    "\n",
    "# if on_colab:\n",
    "#     os.chdir('cancer-proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aug pipelines here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "aug_dict = create_aug_pipelines(size=size,device=device,Augs=BYOL_Augs,TUNE_Augs=TUNE_Augs,Val_Augs=Val_Augs)\n",
    "aug_pipelines = aug_dict['aug_pipelines']\n",
    "aug_pipelines_tune = aug_dict['aug_pipelines_tune']\n",
    "aug_pipelines_test = aug_dict['aug_pipelines_test'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally, display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#show_bt_batch(dls=dls_train,aug=aug_pipelines,n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#show_linear_batch(dls=dls_tune,n_in=3,aug=aug_pipelines_tune,n=2,print_augs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main training api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LM(nn.Module):\n",
    "    \"Basic linear model\"\n",
    "    def __init__(self,encoder):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.head=nn.Linear(2048,9)\n",
    "        if torch.cuda.is_availa\n",
    "        \n",
    "        self.encoder.cuda()\n",
    "        self.head.cuda()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.head(self.encoder(x))\n",
    "\n",
    "#TODO: write nonlinear head here as well if needed. Just need batchnorm, relu another hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def my_splitter(m):\n",
    "    return L(sequential(*m.encoder),m.head).map(params)\n",
    "\n",
    "def my_splitter_bt(m):\n",
    "    return L(sequential(*m.encoder),m.projector).map(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/hamishhaggerty/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#test\u001b[39;00m\n\u001b[1;32m      4\u001b[0m bt_model,encoder \u001b[38;5;241m=\u001b[39m create_model(which_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbt_pretrain\u001b[39m\u001b[38;5;124m'\u001b[39m,ps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test_eq(\u001b[38;5;28mlen\u001b[39m(my_splitter(model)),\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m test_eq(\u001b[38;5;28mlen\u001b[39m(my_splitter_bt(bt_model)),\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mLM.__init__\u001b[0;34m(self, encoder)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m=\u001b[39mencoder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2048\u001b[39m,\u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/cuda/__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "#test\n",
    "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
    "model = LM(encoder)\n",
    "test_eq(len(my_splitter(model)),2)\n",
    "test_eq(len(my_splitter_bt(bt_model)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that splitters are given expected results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. Linear\n",
    "\n",
    "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) head unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. BT\n",
    "\n",
    "learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) projector unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test that parameters are being updated / not updated respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Test that parameters are updated\n",
    "\n",
    "if on_colab: #eventually want to be able to put a flag at top of cell such that can skip if not on colab...\n",
    "\n",
    "    bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
    "    model = LM(encoder)\n",
    "\n",
    "    #Linear\n",
    "    learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
    "    s=list(model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t=model.head.weight.sum()\n",
    "    learn.freeze()\n",
    "    learn.fit_one_cycle(1)\n",
    "    s2=list(model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t2 = model.head.weight.sum()\n",
    "    test_eq(s,s2) #non head parameter shouldn't have been updated\n",
    "    test_ne(t,t2) #head parameter should have been updated\n",
    "\n",
    "    #BT\n",
    "    s=list(bt_model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t=bt_model.projector[0].weight.sum()\n",
    "    learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
    "    learn.fit_one_cycle(1)\n",
    "    s2=list(bt_model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t2=bt_model.projector[0].weight.sum()\n",
    "    test_eq(s,s2) #non head parameter shouldn't have been updated\n",
    "    test_ne(t,t2) #projector parameter should have been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def linear_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Extremely minor change to fine_tune, but seems to work better\"\n",
    "    \n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
    "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, `encoder_fine_tune` is the same as linear_fine_tune, but we can modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Extremely minor change to fine_tune, but seems to work better\"\n",
    "\n",
    "    \n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
    "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class main_train:\n",
    "    \"\"\"Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. \n",
    "    Outputs metrics on validation data\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dls_train, #used for training BT (if pretrain=True)\n",
    "                 dls_tune , #used for tuning\n",
    "                 dls_valid, #used to compute metrics / evaluate results. \n",
    "                 xval, #currently `predict_model` below assumes this is entire validation / test data\n",
    "                 yval,\n",
    "                 aug_pipelines, #the aug pipeline for self-supervised learning\n",
    "                 aug_pipelines_tune, #the aug pipeline for supervised learning\n",
    "                 aug_pipelines_test, #test (or valid) time augmentations \n",
    "                 initial_weights, #Which initial weights to use\n",
    "                 pretrain, #Whether to fit BT\n",
    "                 num_epochs, #number of BT fit epochs\n",
    "                 numfit, #number of tune_fit epochs\n",
    "                 ps=8192, #projection size\n",
    "                 n_in=3, #color channels\n",
    "                 indim=2048, #dimension output of encoder (2048 for resnet50)\n",
    "                 outdim=9, #number of classes\n",
    "                 print_report=False, #F1 metrics etc\n",
    "                 print_plot=False, #ROC curve\n",
    "                 ):\n",
    "                 store_attr()\n",
    "                 self.vocab = self.dls_valid.vocab\n",
    "\n",
    "                 #Soon we might want to save some models here:\n",
    "\n",
    "                 #if self.model_type == 'res_proj': test_eq(self.fit_policy,'resnet_fine_tune') #I THINK this is only viable option?\n",
    "                 #self.encoder_path = f'/content/drive/My Drive/models/baselineencoder_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
    "                 #self.tuned_model_path = f'/content/drive/My Drive/models/baselinefinetuned_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(learn,fit_type,epochs,initial_weights):\n",
    "        \"\"\"We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different\n",
    "        to fine_tune:bt_pretrain\"\"\"\n",
    "\n",
    "        if fit_type == 'encoder_fine_tune': #i.e. barlow twins\n",
    "\n",
    "            learn.encoder_fine_tune(epochs,freeze_epochs=3) \n",
    "\n",
    "        elif fit_type == 'fine_tune':\n",
    "            \n",
    "            #elif initial_weights == 'supervised_pretrain':\n",
    "            learn.linear_fine_tune(epochs,freeze_epochs=1) \n",
    "\n",
    "        else: raise Exception('Fit policy not of expected form')\n",
    "\n",
    "    def train_encoder(self):\n",
    "        \"create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy\"\n",
    "\n",
    "        try: #get existing encoder and plonk on new projector\n",
    "            encoder = self.encoder\n",
    "            encoder.cpu()\n",
    "            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)\n",
    "            bt_model.cuda()\n",
    "\n",
    "        except AttributeError: #otherwise, create\n",
    "            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=device)\n",
    "\n",
    "        if self.pretrain: #train encoder according to fit policy\n",
    "\n",
    "            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=1/self.ps,print_augs=False)])\n",
    "            main_train.fit(learn,fit_type='encoder_fine_tune',epochs=self.num_epochs,initial_weights=self.initial_weights)\n",
    "        self.encoder = bt_model.encoder\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"fine tune in supervised fashion, according to tune_fit_policy, and get metrics\"\n",
    "\n",
    "        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)\n",
    "\n",
    "        try: \n",
    "            encoder = self.encoder\n",
    "        \n",
    "        except AttributeError:\n",
    "            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=device)\n",
    "\n",
    "        model = LM(self.encoder)\n",
    "        learn = Learner(self.dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
    "\n",
    "        #debugging\n",
    "        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
    "\n",
    "        main_train.fit(learn,fit_type='fine_tune',epochs=self.numfit,initial_weights=self.initial_weights) #fine tuning (don't confuse this with fit policy!)\n",
    "        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=3)\n",
    "        #metrics dict will have f1 score, auc etc etc\n",
    "        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)\n",
    "        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)\n",
    "        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval\n",
    "  \n",
    "        #torch.save(model.state_dict(), self.tuned_model_path)\n",
    "        return metrics #\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        self.train_encoder() #train (or extract) the encoder\n",
    "        metrics = self.fine_tune()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "if __name__ == '__main__' and on_colab:\n",
    "\n",
    "    initial_weights = 'supervised_pretrain'\n",
    "    pretrain=True\n",
    "    numfit=1\n",
    "    num_epochs=1\n",
    "\n",
    "    main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
    "            aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
    "            initial_weights=initial_weights,pretrain=pretrain,num_epochs=num_epochs,numfit=numfit,\n",
    "            print_report=True,\n",
    "                    )\n",
    "    \n",
    "    metrics = main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# @patch_to(main_train)\n",
    "# def fit(learn,fit_type,epochs,initial_weights):\n",
    "#     print('hello world!')\n",
    "#     assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# #old supervised baseline (with fine tune)\n",
    "\n",
    "# tem = {0: 0.6724137663841248,\n",
    "#  1: 0.7126436829566956,\n",
    "#  2: 0.6724137663841248,\n",
    "#  3: 0.6321839094161987,\n",
    "#  4: 0.6896551847457886}\n",
    "\n",
    "# from statistics import mean\n",
    "# mean(list(tem.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
