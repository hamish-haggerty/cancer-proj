{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cancer_maintrain\n",
    "\n",
    "> Basic extensible API to train encoder and fine tune. Written in such a way that we can easily patch in modifications as needed. We perform validation / experiments elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cancer_maintrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Surely there is a way to get rid of having to put this cell everywhere. hmmm.\n",
    "\n",
    "Or we can just copy paste / delete this in and out when needed. Either way, getting close to a decent workable workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def colab_is_true():\n",
    "\n",
    "    try: \n",
    "        from google.colab import drive\n",
    "\n",
    "        return True \n",
    "    except ModuleNotFoundError:\n",
    "        return False\n",
    "\n",
    "def setup_colab():\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    #os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"')\n",
    "    os.system('git clone https://github.com/hamish-haggerty/cancer-proj.git')\n",
    "    os.chdir('cancer-proj')\n",
    "    os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"') #does this work?\n",
    "    os.system('pip install -e .')\n",
    "    os.system('pip install -qU nbdev')\n",
    "    os.system('nbdev_install_quarto')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    on_colab = colab_is_true()\n",
    "    if on_colab:\n",
    "        setup_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "from base_rbt.all import *\n",
    "from cancer_proj.cancer_dataloading import *\n",
    "from cancer_proj.cancer_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Since we have cloned repository and cd'd into it (and the data itself is not stored in the\n",
    "#repo) we need cd out of it, get the data, then cd back into the repo `cancer-proj`.\n",
    "#This is a bit annoying, can maybe remove this later\n",
    "if on_colab:\n",
    "    #os.chdir('..') #assumes we are currently in cancer-proj directory\n",
    "    train_dir = colab_train_dir\n",
    "    test_dir = colab_test_dir\n",
    "else:\n",
    "    train_dir = local_train_dir\n",
    "    test_dir = local_test_dir\n",
    "\n",
    "#define general hps\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#bs=256\n",
    "#bs=698\n",
    "bs=256\n",
    "bs_tune=256\n",
    "size=128\n",
    "bs_val=174\n",
    "\n",
    "#get the data dictionary\n",
    "data_dict = get_fnames_dls_dict(train_dir=train_dir,test_dir=test_dir,\n",
    "                    device=device,bs_val=bs_val,bs=bs,bs_tune=bs_tune,size=size,n_in=3)\n",
    "\n",
    "#get the dataloaders\n",
    "dls_train,dls_tune,dls_valid = data_dict['dls_train'],data_dict['dls_tune'],data_dict['dls_valid']\n",
    "x,y = data_dict['x'],data_dict['y']\n",
    "xval,yval = data_dict['xval'],data_dict['yval']\n",
    "xtune,ytune = data_dict['xtune'],data_dict['ytune']\n",
    "vocab = data_dict['vocab']\n",
    "\n",
    "#If we want to write some tests (make sure the data is same every time etc):\n",
    "fnames,fnames_train,fnames_tune,fnames_valid,fnames_test = data_dict['fnames'],data_dict['fnames_train'],data_dict['fnames_tune'],data_dict['fnames_valid'],data_dict['fnames_test']\n",
    "\n",
    "test_eq(x.shape,xtune.shape)\n",
    "\n",
    "# if on_colab:\n",
    "#     os.chdir('cancer-proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aug pipelines here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "aug_dict = create_aug_pipelines(size=size,device=device,Augs=BYOL_Augs,TUNE_Augs=TUNE_Augs,Val_Augs=Val_Augs)\n",
    "aug_pipelines = aug_dict['aug_pipelines']\n",
    "aug_pipelines_tune = aug_dict['aug_pipelines_tune']\n",
    "aug_pipelines_test = aug_dict['aug_pipelines_test'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally, display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#show_bt_batch(dls=dls_train,aug=aug_pipelines,n_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#show_linear_batch(dls=dls_tune,n_in=3,aug=aug_pipelines_tune,n=2,print_augs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main training api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LM(nn.Module):\n",
    "    \"Basic linear model\"\n",
    "    def __init__(self,encoder,numout):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.head=nn.Linear(2048,numout)\n",
    "        if torch.cuda.is_available():\n",
    "            self.encoder.cuda()\n",
    "            self.head.cuda()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.head(self.encoder(x))\n",
    "\n",
    "#TODO: write nonlinear head here as well if needed. Just need batchnorm, relu another hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def my_splitter(m):\n",
    "    return L(sequential(*m.encoder),m.head).map(params)\n",
    "\n",
    "def my_splitter_bt(m):\n",
    "    return L(sequential(*m.encoder),m.projector).map(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test\n",
    "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
    "model = LM(encoder,numout=10)\n",
    "test_eq(len(my_splitter(model)),2)\n",
    "test_eq(len(my_splitter_bt(bt_model)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that splitters are given expected results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. Linear\n",
    "\n",
    "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) head unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. BT\n",
    "\n",
    "learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) projector unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test that parameters are being updated / not updated respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Test that parameters are updated\n",
    "\n",
    "if on_colab: #eventually want to be able to put a flag at top of cell such that can skip if not on colab...\n",
    "\n",
    "    bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
    "    model = LM(encoder,numout=9)\n",
    "\n",
    "    #Linear\n",
    "    learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
    "    s=list(model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t=model.head.weight.sum()\n",
    "    learn.freeze()\n",
    "    learn.fit_one_cycle(1)\n",
    "    s2=list(model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t2 = model.head.weight.sum()\n",
    "    test_eq(s,s2) #non head parameter shouldn't have been updated\n",
    "    test_ne(t,t2) #head parameter should have been updated\n",
    "\n",
    "    #BT\n",
    "    s=list(bt_model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t=bt_model.projector[0].weight.sum()\n",
    "    learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
    "    learn.fit_one_cycle(1)\n",
    "    s2=list(bt_model.encoder[5][0].conv1.parameters())[0].sum().item() #sum up some parameters that shouldn't change\n",
    "    t2=bt_model.projector[0].weight.sum()\n",
    "    test_eq(s,s2) #non head parameter shouldn't have been updated\n",
    "    test_ne(t,t2) #projector parameter should have been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def linear_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Extremely minor change to fine_tune, but seems to work better\"\n",
    "    \n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
    "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, `encoder_fine_tune` is the same as linear_fine_tune, but we can modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Extremely minor change to fine_tune, but seems to work better\"\n",
    "\n",
    "    \n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
    "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class main_train:\n",
    "    \"\"\"Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. \n",
    "    Outputs metrics on validation data\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dls_train, #used for training BT (if pretrain=True)\n",
    "                 dls_tune , #used for tuning\n",
    "                 dls_valid, #used to compute metrics / evaluate results. \n",
    "                 xval, #currently `predict_model` below assumes this is entire validation / test data\n",
    "                 yval,\n",
    "                 aug_pipelines, #the aug pipeline for self-supervised learning\n",
    "                 aug_pipelines_tune, #the aug pipeline for supervised learning\n",
    "                 aug_pipelines_test, #test (or valid) time augmentations \n",
    "                 initial_weights, #Which initial weights to use\n",
    "                 pretrain, #Has the model been pretrained?\n",
    "                 num_epochs, #number of BT fit epochs\n",
    "                 numfit, #number of tune_fit epochs\n",
    "                 freeze_num_epochs, #How many epochs to freeze body for when training BT\n",
    "                 freeze_numfit, #How many epochs to freeze body for when fine tuning\n",
    "                 ps=8192, #projection size\n",
    "                 n_in=3, #color channels\n",
    "                 indim=2048, #dimension output of encoder (2048 for resnet50)\n",
    "                 outdim=9, #number of classes\n",
    "                 lr_max=None,#maximum learning rate used in `fit_one_cycle` (training encoder)\n",
    "                 lmb=None, #generally in {1/8192, 0.5*5e-3, 5e-3} at the moment\n",
    "                 print_report=False, #F1 metrics etc\n",
    "                 print_plot=False, #ROC curve\n",
    "                 tune_model_path=None, #save models after fine tuning\n",
    "                 ):\n",
    "        store_attr()\n",
    "        self.vocab = self.dls_valid.vocab\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    " \n",
    "    @staticmethod\n",
    "    def fit(learn,fit_type,epochs,freeze_epochs,initial_weights,pretrain,lr_max=None):\n",
    "        \"\"\"We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different\n",
    "        to fine_tune:bt_pretrain\"\"\"\n",
    "        \n",
    "        #Ok, just use `fit` for everything: keep it simple. Only wrinkle is we freeze\n",
    "        #body when fine tuning IF model has been pretrained\n",
    "\n",
    "\n",
    "        if fit_type == 'encoder_fine_tune': #i.e. barlow twins\n",
    "            learn.fit_one_cycle(epochs,lr_max)\n",
    "\n",
    "        elif fit_type == 'fine_tune':\n",
    "\n",
    "            if pretrain == False and initial_weights not in ['bt_pretrain','supervised_pretrain']:\n",
    "\n",
    "                learn.fit(epochs)\n",
    "\n",
    "            else:\n",
    "                learn.freeze()\n",
    "                print('froze body')\n",
    "                learn.fit(freeze_epochs)\n",
    "                learn.unfreeze()\n",
    "                print('unfroze body')\n",
    "                learn.fit(epochs)\n",
    "    \n",
    "        else: raise Exception('Fit policy not of expected form')\n",
    "        \n",
    "\n",
    "    def train_encoder(self):\n",
    "        \"create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy\"\n",
    "\n",
    "        try: #get existing encoder and plonk on new projector\n",
    "            encoder = self.encoder\n",
    "            encoder.cpu()\n",
    "            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)\n",
    "            bt_model.cuda()\n",
    "\n",
    "        except AttributeError: #otherwise, create\n",
    "            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)\n",
    "\n",
    "        if self.pretrain: #train encoder according to fit policy\n",
    "\n",
    "            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=self.lmb,print_augs=False)])\n",
    "            main_train.fit(learn,fit_type='encoder_fine_tune',\n",
    "                           epochs=self.num_epochs,freeze_epochs=self.freeze_num_epochs,\n",
    "                           initial_weights=self.initial_weights,\n",
    "                           pretrain=self.pretrain,\n",
    "                           lr_max=self.lr_max\n",
    "                          )\n",
    "            \n",
    "        self.encoder = bt_model.encoder\n",
    "        self.bt_model = bt_model\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"fine tune in supervised fashion, according to tune_fit_policy, and get metrics\"\n",
    "\n",
    "        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)\n",
    "\n",
    "        try: \n",
    "            encoder = self.encoder\n",
    "        \n",
    "        except AttributeError:\n",
    "            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)\n",
    "\n",
    "        model = LM(self.encoder,numout=len(self.vocab))\n",
    "        learn = Learner(self.dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
    "        \n",
    "        #debugging \n",
    "        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
    "\n",
    "        main_train.fit(learn,fit_type='fine_tune',\n",
    "                       epochs=self.numfit,freeze_epochs=self.freeze_numfit,\n",
    "                       initial_weights=self.initial_weights,\n",
    "                       pretrain=self.pretrain\n",
    "                      ) #fine tuning (don't confuse this with fit policy!)\n",
    "        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=10)\n",
    "        #metrics dict will have f1 score, auc etc etc\n",
    "        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)\n",
    "        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)\n",
    "        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval\n",
    "  \n",
    "\n",
    "        if self.tune_model_path != None:\n",
    "            metrics['classif_model_path'] = self.tune_model_path\n",
    "            torch.save(model.state_dict(), self.tune_model_path)\n",
    "        return metrics #\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        self.train_encoder() #train (or extract) the encoder\n",
    "        metrics = self.fine_tune()\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# @patch_to(main_train)\n",
    "# def fit(learn,fit_type,epochs,initial_weights):\n",
    "#     print('hello world!')\n",
    "#     assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
