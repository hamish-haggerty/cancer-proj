{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cancer_maintrain\n",
    "\n",
    "   > Basic extensible API to train encoder and fine tune. Written in such a way that we can easily patch in modifications as needed. We perform validation / experiments elsewhere. Note also that we have a `predict_whole_model` function at the ends rather than in `metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cancer_maintrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Surely there is a way to get rid of having to put this cell everywhere. hmmm.\n",
    "\n",
    "Or we can just copy paste / delete this in and out when needed. Either way, getting close to a decent workable workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "from base_rbt.all import *\n",
    "from cancer_proj.cancer_dataloading import *\n",
    "from cancer_proj.cancer_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main training api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LM(nn.Module):\n",
    "    \"Basic linear model\"\n",
    "    def __init__(self,encoder,numout,numin=2048):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.head=nn.Linear(numin,numout)\n",
    "        if torch.cuda.is_available():\n",
    "            self.encoder.cuda()\n",
    "            self.head.cuda()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.head(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def my_splitter(m):\n",
    "    return L(sequential(*m.encoder),m.head).map(params)\n",
    "\n",
    "def my_splitter_bt(m):\n",
    "    return L(sequential(*m.encoder),m.projector).map(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify splitter works on CIFAR data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "path = untar_data(URLs.CIFAR)\n",
    "def label_func(fname):\n",
    "    return fname.name.split('_')[1].strip('png').strip('.')\n",
    "fnames = get_image_files(path / \"train\")\n",
    "fnames = fnames.shuffle()\n",
    "fnames = fnames[0:100]\n",
    "labels = [label_func(fname) for fname in fnames]\n",
    "_dls = ImageDataLoaders.from_lists(path, fnames, labels,bs=100, #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=0,device=device)\n",
    "\n",
    "\n",
    "#test\n",
    "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
    "model = LM(encoder,numout=10)\n",
    "test_eq(len(my_splitter(model)),2)\n",
    "test_eq(len(my_splitter_bt(bt_model)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that splitters are given expected results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. Linear\n",
    "\n",
    "\n",
    "\n",
    "learn = Learner(_dls,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) head unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#test : manual. BT\n",
    "\n",
    "learn = Learner(_dls,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
    "learn.freeze()\n",
    "print('body should be frozen, (sans batchnorm) projector unfrozen')\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "print('should be all unfrozen')\n",
    "learn.unfreeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LinearBt(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self,aug_pipelines,n_in, show_batch=False, print_augs=False,data=None,\n",
    "                 tune_model_path=None,tune_save_after=None):\n",
    "        self.aug1= aug_pipelines[0]\n",
    "        self.aug2=Pipeline( split_idx = 0) #empty pipeline\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        self.n_in=n_in\n",
    "        self._show_batch=show_batch\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.data=data #if data is just e.g. 20 samples then don't bother re-loading each time\n",
    "        self.tune_model_path=tune_model_path\n",
    "        self.tune_save_after = tune_save_after\n",
    "\n",
    "\n",
    "    def after_create(self):\n",
    "        self.learn.tune_model_path_dict = {}\n",
    "        self.learn.tune_model_path=self.tune_model_path\n",
    "\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learn.loss_func = self.lf\n",
    "            \n",
    "    def before_batch(self):\n",
    "\n",
    "        if self.n_in == 1:\n",
    "            xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))                            \n",
    "        elif self.n_in == 3:\n",
    "            xi,xj = self.aug1(TensorImage(self.x)), self.aug2(TensorImage(self.x))\n",
    "        self.learn.xb = (xi,)\n",
    "\n",
    "        if self._show_batch:\n",
    "            self.learn.aug_x = torch.cat([xi, xj])\n",
    "\n",
    "            \n",
    "    def after_epoch(self):\n",
    "        \n",
    "        true_epoch = self.epoch+1\n",
    "        \n",
    "        if true_epoch%self.tune_save_after == 0 and self.learn.tune_model_path!=None:\n",
    "            #self.learn.tune_path = self.learn.tune_path +f'_epochs={self.n_epoch//50}'\n",
    "            #path = self.learn.tune_model_path + f'_epochs={true_epoch}'\n",
    "            \n",
    "            path = self.learn.tune_model_path\n",
    "            print(f'We are saving after true epoch {true_epoch} at path {path}')\n",
    "            torch.save(self.learn.model.state_dict(), path)\n",
    "            #self.learn.tune_model_path_dict[true_epoch]=path\n",
    "\n",
    "\n",
    "    def lf(self, pred, *yb):        \n",
    "        loss=self.criterion(pred,self.y)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        if self._show_batch==False:\n",
    "            print('Need to set show_batch=True')\n",
    "            return\n",
    "        bs = self.learn.aug_x.size(0)//2\n",
    "        x1,x2  = self.learn.aug_x[:bs], self.learn.aug_x[bs:]\n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]]\n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def fine_tune(initial_weights,dls_tune,device,encoder=None,epochs=40,tune_model_path=None):\n",
    "    \n",
    "    \n",
    "    if encoder is None: #Generally speaking, this will be None, unless we pretrained an encoder somewhere else and want to pass it in\n",
    "        _,encoder = create_model(initial_weights,device) #either bt_pretrain, supervised_pretrain etc\n",
    "    \n",
    "\n",
    "    print(f'tune_model_path for this run is: {tune_model_path}')\n",
    "    model = LM(encoder,numout=len(dls_tune.vocab))\n",
    "    learn = Learner(dls_tune,model,splitter=my_splitter,\n",
    "                        cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,\n",
    "                                        n_in=3,tune_model_path=tune_model_path, #if None then don't save\n",
    "                                    tune_save_after=epochs)],wd=0.0\n",
    "                        \n",
    "                   )\n",
    "    \n",
    "    if initial_weights!='no_pretrain': #Means we are in transfer learning setting\n",
    "        learn.freeze()\n",
    "        print('Froze head')\n",
    "        learn.fit(1)\n",
    "        learn.unfreeze()\n",
    "        print('Unfroze head')\n",
    "    \n",
    "    lrs = learn.lr_find()\n",
    "    lr_max=lrs.valley\n",
    "    print(f'Learning rate finder yielded lr_max: {lr_max}')\n",
    "    learn.fit_one_cycle(epochs,lr_max)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dls_metrics(dls,model,int_to_classes): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from model and dataloader\"\n",
    "\n",
    "    ytest,probs,preds,Acc = predict_whole_model(dls,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, ytest,int_to_classes, print_report=True)\n",
    "    \n",
    "    auc_dict = plot_roc(ytest,preds,int_to_classes,print_plot=True)\n",
    "    metrics['ytest']=ytest\n",
    "    metrics['probs']=probs\n",
    "    metrics['preds']=preds\n",
    "    metrics['acc']=Acc\n",
    "    metrics['auc_dict']=auc_dict\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_xval_metrics(xval,yval,model,aug_pipelines_test,int_to_classes,numavg=3): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from gives batch (xval,yval)\"\n",
    "\n",
    "    probs,preds,Acc = predict_model(xval,yval,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, yval,int_to_classes, print_report=True)\n",
    "    metrics['acc']=Acc\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def Mean_Results(results):\n",
    "    \"Get mean classif report and display it\"\n",
    "\n",
    "    lst = list(vocab) + ['accuracy', 'macro avg', 'weighted avg']\n",
    "    reports=[]\n",
    "    accs=[]\n",
    "    for i in results.keys():\n",
    "        if type(i)!=int:\n",
    "            continue\n",
    "        report = {j:results[i][j] for j in results[i].keys() if j in lst}\n",
    "        reports.append(report)\n",
    "        accs.append(results[i]['acc'])\n",
    "    mean_report = Mean_Report(reports,vocab)\n",
    "    print(format_classification_report(mean_report))\n",
    "    \n",
    "    print(f'mean acc is {mean(accs)} with std {stdev(accs)}')\n",
    "\n",
    "    return mean_report\n",
    "\n",
    "#fine tune, return the model and path\n",
    "\n",
    "def main_tune(initial_weights,epochs=40,device='cuda',\n",
    "              encoder=None,tune_model_path=None,dict_path=None,description=None,\n",
    "              results=None,runs=range(1)\n",
    "             ):\n",
    "\n",
    "    \"Fine tune and save  test results for supervised or bt initial weights\"\n",
    "\n",
    "    if description == None:\n",
    "        description=f'Fine tuned {weights} initial weights for 40 epochs. Recorded results on test sets. Did this {runs} times'\n",
    "\n",
    "    if dict_path==None:\n",
    "        dict_path=f'{weights}_results'\n",
    "\n",
    "    weights = initial_weights.split('_')[0]\n",
    "\n",
    "    print(f'Description: {description}\\n')\n",
    "    print(f'The general tune model path is: {tune_model_path} (if None mean no saving)')\n",
    "    print(f'The dict_path is: {dict_path}')\n",
    "    \n",
    "    if results==None:\n",
    "        results={}\n",
    "    \n",
    "    for i in runs:\n",
    "\n",
    "        _tune_model_path = tune_model_path + f'_run{i}'\n",
    "\n",
    "        fine_tuned = fine_tune(initial_weights,dls_tune,device,encoder=encoder,epochs=epochs,tune_model_path=_tune_model_path)\n",
    "\n",
    "        #get the metrics\n",
    "        metrics = get_dls_metrics(dls_test,fine_tuned,int_to_classes)\n",
    "        print(metrics['acc'])\n",
    "\n",
    "        #put the path in in the metrics and a short description\n",
    "        metrics['tune_model_path'],metrics['description'] = tune_model_path,description\n",
    "\n",
    "        results[i] = metrics\n",
    "\n",
    "    #save\n",
    "    if tune_model_path!=None:\n",
    "        print(f'We are saving the dictionary at {dict_path}') #this is a bug. We saved at f'{weights}_results'\n",
    "        save_dict_to_gdrive(results,save_directory,dict_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_whole_model(dls_test, model, aug_pipelines_test, numavg=3, criterion=CrossEntropyLossFlat(), deterministic=False):\n",
    "    \"\"\"\n",
    "    Predicts the labels and probabilities for the entire test set using the specified model and data augmentation\n",
    "    pipelines. Returns a dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "\n",
    "    Args:\n",
    "        dls_test: The test dataloader.\n",
    "        model: The trained model.\n",
    "        aug_pipelines_test: The test data augmentation pipelines.\n",
    "        numavg: The number of times to perform test-time augmentation.\n",
    "        criterion: The loss function to use for computing the accuracy.\n",
    "        deterministic: Whether to use deterministic computation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_len = len(dls_test.dataset)\n",
    "    y = torch.zeros(total_len, dtype=torch.long)\n",
    "    probs = torch.zeros(total_len, model.head.out_features)\n",
    "    ypred = torch.zeros(total_len, dtype=torch.long)\n",
    "\n",
    "    start_idx = 0\n",
    "    for xval, yval in dls_test.train:\n",
    "        end_idx = start_idx + len(xval)\n",
    "        _probs, _ypred, acc = predict_model(xval, yval, model, aug_pipelines_test, numavg, criterion, deterministic)\n",
    "        y[start_idx:end_idx] = yval\n",
    "        probs[start_idx:end_idx] = _probs\n",
    "        ypred[start_idx:end_idx] = _ypred\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    acc = (ypred == y).float().mean().item()\n",
    "\n",
    "    # Return the predictions and labels in a dictionary\n",
    "    #return {'y': y, 'probs': probs, 'ypred': ypred, 'acc': acc}\n",
    "    return y,probs,ypred,acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test that `predict_whole_model` gives expected results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test, we need determinism: hence we need a dataloader where there isn't random resizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "_xval,_yval = _dls.train.one_batch()\n",
    "\n",
    "#acc using `predict_model`\n",
    "probs,ypred,acc = predict_model(xval=_xval,yval=_yval,model=model,\n",
    "                      aug_pipelines_test=aug_pipelines_test,deterministic=True\n",
    "                               )\n",
    "\n",
    "_dls = ImageDataLoaders.from_lists(path, fnames, labels,bs=50, #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=0,device=device)\n",
    "#acc using `predict_whole_model` when bs = 1/2 the size of dataset \n",
    "y,Probs,Ypred,Acc = predict_whole_model(_dls,model,aug_pipelines_test,deterministic=True)\n",
    "\n",
    "test_eq(abs(Acc-acc)<1e-7,True) #test that both methods give same acc, up to rounding\n",
    "\n",
    "test_eq(abs(probs[:,3].sum().item()-Probs[:,3].sum().item())<1e-3,True) #test that both methods give same probs (scores), up to rounding\n",
    "        \n",
    "test_eq(ypred.sum(),Ypred.sum()) #test that both methods give same preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
