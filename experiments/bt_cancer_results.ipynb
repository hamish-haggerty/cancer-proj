{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFyID8b0JRX0"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k862N9EPayst",
        "outputId": "d215271c-aadc-4412-9b33-f85962329b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "!unzip -q \"/content/drive/MyDrive/isic-2019.zip\" #https://www.kaggle.com/datasets/andrewmvd/isic-2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Hui8nFuOSt",
        "outputId": "d30a0551-ab2e-483b-f19c-496c76baecb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#os.system('pip install .')\n",
        "os.system('pip install git+https://github.com/hamish-haggerty/base_rbt.git')\n",
        "os.system('pip install git+https://github.com/hamish-haggerty/cancer-proj.git')\n",
        "!pip install -qU git+https://github.com/hamish-haggerty/cancer-proj.git #sometimes os doesn't work so use this if imports below fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Flz66CWEuQSA"
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "from fastai.vision.all import *\n",
        "from base_rbt.all import *\n",
        "#TODO: wrap this in an .all\n",
        "from cancer_proj.cancer_dataloading import *\n",
        "from cancer_proj.cancer_metrics import *\n",
        "from cancer_proj.cancer_maintrain import *\n",
        "\n",
        "from self_supervised.augmentations import assert_aug_pipelines\n",
        "from self_supervised.layers import create_mlp_module\n",
        "from statistics import mean,stdev\n",
        "\n",
        "import fastai\n",
        "test_eq(fastai.__version__,'2.7.11')\n",
        "\n",
        "import torch\n",
        "test_eq(torch.__version__,'1.13.1+cu116')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R-Ljdd32jhv"
      },
      "source": [
        "#Get data paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FPWONj0-867g"
      },
      "outputs": [],
      "source": [
        "save_directory = '/content/drive/My Drive/cancer_colab' #directory for saving models etc\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-FOBPUdtStHm"
      },
      "outputs": [],
      "source": [
        "directory = \"/content/drive/MyDrive/ISIC_2019_Training_Input/\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ISIC_2019_Training_GroundTruth.csv\").drop(\"UNK\", axis=1)\n",
        "data = data[~data[\"image\"].str.contains(\"downsampled\")]\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/ISIC_2019_Training_GroundTruth.csv\")\n",
        "\n",
        "data_dict = load_dict_from_gdrive(directory=save_directory,filename='data_dict') \n",
        "_fnames = data_dict['_fnames']\n",
        "#_fnames = get_image_files(directory) \n",
        "#_fnames = [name for name in _fnames if 'downsampled' not in name.as_posix()] #otherwise load like this\n",
        "test_eq(len(_fnames),len(data))\n",
        "\n",
        "label_func_dict = data_dict['label_func_dict']\n",
        "\n",
        "def label_func(name):\n",
        "    return label_func_dict[name]\n",
        "_labels = [label_func(i) for i in _fnames]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHcpj14q2vWl"
      },
      "source": [
        "# Build training, tuning, validation test sets:\n",
        "    - Training is unlabelled\n",
        "    - Tuning is for supervised fine tuning\n",
        "    - Validation is a held out (proxy) test set\n",
        "    - Test set is for getting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xad_IxK2PcZd",
        "outputId": "c7a52be4-b907-480d-d3b8-d3caacadfbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training (tuning) set has: \n",
            "Counter({'NV': 500, 'MEL': 500, 'BCC': 500, 'BKL': 467, 'AK': 306, 'SCC': 171, 'VASC': 55, 'DF': 55})\n",
            "\n",
            "Validation set has: \n",
            "Counter({'NV': 458, 'MEL': 309, 'BCC': 274, 'BKL': 110, 'AK': 63, 'SCC': 43, 'VASC': 12, 'DF': 11})\n",
            "\n",
            "Test set has: \n",
            "Counter({'NV': 10601, 'MEL': 3339, 'BCC': 2549, 'BKL': 1663, 'AK': 498, 'SCC': 414, 'VASC': 186, 'DF': 173})\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#tests / sanity checks:\n",
        "test_eq(process_path(_fnames[0]),'ISIC_0071718.jpg')\n",
        "test_eq(process_path(_fnames[10]),'ISIC_0071719.jpg')\n",
        "\n",
        "_fnames_dict = get_fnames(_fnames,_labels,label_func)\n",
        "fnames_train,fnames_valid,fnames_test = _fnames_dict['fnames_train'],_fnames_dict['fnames_valid'],_fnames_dict['fnames_test'] \n",
        "labels_train,labels_valid,labels_test = _fnames_dict['labels_train'],_fnames_dict['labels_valid'],_fnames_dict['labels_test'] \n",
        "\n",
        "print(f'Training (tuning) set has: \\n{Counter(labels_train)}\\n')\n",
        "\n",
        "print(f'Validation set has: \\n{Counter(labels_valid)}\\n')\n",
        "\n",
        "print(f'Test set has: \\n{Counter(labels_test)}\\n')\n",
        "\n",
        "#A few tests: Make sure fnames_train and fnames_test the same every time\n",
        "test_eq(process_path(fnames_train[44]),'ISIC_0071754.jpg')\n",
        "test_eq(process_path(fnames_test[10]),'ISIC_0000011.jpg')\n",
        "\n",
        "#Make sure training and valid are disjoint\n",
        "for path in fnames_valid: assert path not in fnames_train #check that valid set is disjoint from training (tuning) set\n",
        "\n",
        "#Make sure test and valid+train are disjoint\n",
        "for path in fnames_test: assert path not in fnames_train+fnames_valid #check that test set is disticnt from training and validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHAvOiNDOMJc"
      },
      "source": [
        "# Setup dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEoL_IvqPtDS"
      },
      "outputs": [],
      "source": [
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "size=256\n",
        "item_tfms = [Resize(size)]\n",
        "\n",
        "item_tfms_train = [Resize(128)]\n",
        "\n",
        "dls_tune  = ImageDataLoaders.from_path_func(directory, fnames_train, label_func,\n",
        "                                bs=64,\n",
        "                                item_tfms=item_tfms,\n",
        "                                valid_pct=0,\n",
        "                                device=device,\n",
        "                                num_workers=12*(device=='cuda')\n",
        "                                             )\n",
        "\n",
        "\n",
        "dls_valid  = ImageDataLoaders.from_path_func(directory, fnames_valid, label_func,\n",
        "                                bs=256,\n",
        "                                item_tfms=item_tfms,\n",
        "                                valid_pct=0,\n",
        "                                device=device,\n",
        "                                num_workers=12*(device=='cuda')\n",
        "                                             )\n",
        "\n",
        "#This is for training BT (so viewed as unlabelled)\n",
        "dls_train  = ImageDataLoaders.from_path_func(directory, fnames_train, label_func,\n",
        "                                bs=256,\n",
        "                                item_tfms=item_tfms_train,\n",
        "                                valid_pct=0,\n",
        "                                device=device,\n",
        "                                num_workers=12*(device=='cuda')\n",
        "                                             )\n",
        "\n",
        "\n",
        "dls_test =  ImageDataLoaders.from_path_func(directory, fnames_test, label_func,\n",
        "                                bs=64,\n",
        "                                item_tfms=item_tfms,\n",
        "                                valid_pct=0,\n",
        "                                device=device,\n",
        "                                num_workers=12*(device=='cuda'),\n",
        "                                shuffle=False\n",
        "                                )\n",
        "\n",
        "classes_to_int={v:i for i,v in enumerate(dls_tune.vocab)}\n",
        "int_to_classes = {i: v for i, v in enumerate(dls_tune.vocab)}\n",
        "vocab=dls_tune.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkxss---YsbP"
      },
      "source": [
        "# Debugging / verify that works:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53CBSqIsQKw_"
      },
      "source": [
        "# Aug pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbTmv3IATdYh"
      },
      "outputs": [],
      "source": [
        "aug_dict = create_aug_pipelines(size=size,device=device,Augs=BYOL_Augs,TUNE_Augs=TUNE_Augs,Val_Augs=Val_Augs)\n",
        "aug_pipelines = aug_dict['aug_pipelines'] #Heavy augmentation. Use to train BT\n",
        "aug_pipelines_tune = aug_dict['aug_pipelines_tune'] #Used for fine tuning\n",
        "aug_pipelines_test = aug_dict['aug_pipelines_test'] #Test time augmentation (generally same as above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoVJJiTfgDz8"
      },
      "outputs": [],
      "source": [
        "#show_bt_batch(dls_train,n_in=3,n=2,aug=aug_pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gGuUxBXyZss"
      },
      "source": [
        "# Let's explore whether pretraining a second time helps performance (pre-pre training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dtwVNwFzsPD"
      },
      "outputs": [],
      "source": [
        "#Set the training dataloader for BT equal to all the available data\n",
        "#_fnames_train = fnames_train+fnames_valid\n",
        "#_fnames_train = [name for name in fnames_train if label_func(name)=='SCC']\n",
        "#_fnames_train = fnames_train + fnames_valid\n",
        "\n",
        "_fnames_train=fnames_train+fnames_test\n",
        "\n",
        "dls_train  = ImageDataLoaders.from_path_func(directory,_fnames_train, label_func,\n",
        "                                bs=128,\n",
        "                                item_tfms=item_tfms,\n",
        "                                valid_pct=0,\n",
        "                                device=device,\n",
        "                                num_workers=12*(device=='cuda')\n",
        "                                             )\n",
        "#Need to lower size for memory requirements\n",
        "_aug_dict = create_aug_pipelines(size=256,device=device,Augs=BYOL_Augs,TUNE_Augs=TUNE_Augs,Val_Augs=Val_Augs)\n",
        "aug_pipelines = aug_dict['aug_pipelines'] #Heavy augmentation. Use to train BT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AfVcM7RFe0R"
      },
      "outputs": [],
      "source": [
        "# bt_model,_ = create_model('bt_pretrain',device)\n",
        "# learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
        "\n",
        "# learn.freeze()\n",
        "# #learn.summary() #You can call this block to verify that the the encoder is being frozen\n",
        "# learn.fit(10)\n",
        "# learn.unfreeze()\n",
        "# lrs = learn.lr_find()\n",
        "# print(lrs.valley)\n",
        "# learn.fit_one_cycle(500,lrs.valley)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2XgLwvNQ3x6"
      },
      "source": [
        "# Train Barlow Twins and fine tune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bt(freeze_epochs,epochs):\n",
        "    \"Train barlow twins\"\n",
        "\n",
        "    bt_model,_ = create_model('bt_pretrain',device)\n",
        "    learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
        "\n",
        "    learn.freeze()\n",
        "    #learn.summary() #You can call this block to verify that the the encoder is being frozen\n",
        "    learn.fit(freeze_epochs)\n",
        "    learn.unfreeze()\n",
        "    lrs = learn.lr_find()\n",
        "    print(lrs.valley)\n",
        "    learn.fit_one_cycle(epochs,lrs.valley)\n",
        "\n",
        "    return learn"
      ],
      "metadata": {
        "id": "WFKxXizwZmgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aYQgZcs-dSzd"
      },
      "outputs": [],
      "source": [
        "description = 'We pretrained BT initial weights on train+valid. lmb=1/8192,bs=128,resize=128'\n",
        "initial_weights='bt_prepretrained' #pretrain again, hence `prepre`\n",
        "epochs=40\n",
        "tune_model_path = save_directory + f'/initial_weights={initial_weights}'\n",
        "dict_path = f'{initial_weights}' #file to load dictionary metadata\n",
        "#########\n",
        "\n",
        "_runs=range(5)\n",
        "for i in _runs:\n",
        "\n",
        "    #Train barlow twins.\n",
        "    _learn = train_bt(freeze_epochs=1,epochs=100)\n",
        "\n",
        "    #load results\n",
        "    _results=None\n",
        "    if i>0: _results = load_dict_from_gdrive(save_directory,'bt_prepretrained')\n",
        "\n",
        "    runs=[i]\n",
        "    #fine tune\n",
        "    btprpre_results = main_tune(encoder = _learn.model.encoder,\n",
        "                            initial_weights=initial_weights,epochs=epochs,device=device,dls_tune=dls_tune,dls_test=dls_test,\n",
        "                            aug_pipelines_tune=aug_pipelines_tune,aug_pipelines_test=aug_pipelines_tune,int_to_classes=int_to_classes,\n",
        "                            tune_model_path=tune_model_path, dict_path = dict_path,save_directory=save_directory,description=description,\n",
        "                            results=_results,runs=runs,\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_results = load_dict_from_gdrive(save_directory,'bt_prepretrained')\n",
        "lst=[]\n",
        "for k in _results:\n",
        "    print(k)\n",
        "    print(_results[k]['acc'])\n",
        "    lst.append(_results[k]['acc'])\n",
        "\n",
        "print('\\n')\n",
        "print(mean(lst))\n",
        "print(stdev(lst))"
      ],
      "metadata": {
        "id": "y56Ha3nmZ_kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac71d054-8771-4c5d-edb1-3093f17f5219"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.7280029058456421\n",
            "1\n",
            "0.7199196815490723\n",
            "2\n",
            "0.7186840176582336\n",
            "3\n",
            "0.7226483821868896\n",
            "4\n",
            "0.7207949161529541\n",
            "5\n",
            "0.7260979413986206\n",
            "\n",
            "\n",
            "0.7226913074652354\n",
            "0.003664878074809444\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}