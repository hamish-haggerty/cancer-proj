# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/cancer_maintrain.ipynb.

# %% auto 0
__all__ = ['DotDict', 'LM', 'my_splitter', 'my_splitter_bt', 'LinearBt', 'main_train', 'predict_model', 'predict_whole_model']

# %% ../nbs/cancer_maintrain.ipynb 5
from fastai.vision.all import *
from base_rbt.all import *
from .cancer_dataloading import *
from .cancer_metrics import *

# %% ../nbs/cancer_maintrain.ipynb 6
class DotDict(dict):
    def __getattr__(self, key):
        if key in self:
            return self[key]
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{key}'")


# %% ../nbs/cancer_maintrain.ipynb 15
@patch
def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred,I=self.I,lmb=self.lmb)

# %% ../nbs/cancer_maintrain.ipynb 17
class LM(nn.Module):
    "Basic linear model"
    def __init__(self,encoder,numout):
        super().__init__()
        self.encoder=encoder
        self.head=nn.Linear(2048,numout)
        if torch.cuda.is_available():
            self.encoder.cuda()
            self.head.cuda()

    def forward(self,x):
        return self.head(self.encoder(x))

#TODO: write nonlinear head here as well if needed. Just need batchnorm, relu another hidden layer

# %% ../nbs/cancer_maintrain.ipynb 18
def my_splitter(m):
    return L(sequential(*m.encoder),m.head).map(params)

def my_splitter_bt(m):
    return L(sequential(*m.encoder),m.projector).map(params)


# %% ../nbs/cancer_maintrain.ipynb 30
@patch
@delegates(Learner.fit_one_cycle)
def linear_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,
              pct_start=0.3, div=5.0, **kwargs):
    "Extremely minor change to fine_tune, but seems to work better"
    
    self.freeze()
    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)
    base_lr /= 2
    self.unfreeze()
    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)
    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)

# %% ../nbs/cancer_maintrain.ipynb 32
@patch
@delegates(Learner.fit_one_cycle)
def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,
              pct_start=0.3, div=5.0, **kwargs):
    "Extremely minor change to fine_tune, but seems to work better"

    
    self.freeze()
    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)
    base_lr /= 2
    self.unfreeze()
    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)


# %% ../nbs/cancer_maintrain.ipynb 33
class LinearBt(Callback):
    order,run_valid = 9,True
    def __init__(self,aug_pipelines,n_in, show_batch=False, print_augs=False,data=None,tune_path=None):
        self.aug1= aug_pipelines[0]
        self.aug2=Pipeline( split_idx = 0) #empty pipeline
        if print_augs: print(self.aug1), print(self.aug2)
        self.n_in=n_in
        self._show_batch=show_batch
        self.criterion = nn.CrossEntropyLoss()
        self.data=data #if data is just e.g. 20 samples then don't bother re-loading each time
        self.tune_path=tune_path


    def after_create(self):
        self.learn.tune_path = self.tune_path
    

    def before_fit(self):
        self.learn.loss_func = self.lf
            
    def before_batch(self):

        if self.n_in == 1:
            xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))                            
        elif self.n_in == 3:
            xi,xj = self.aug1(TensorImage(self.x)), self.aug2(TensorImage(self.x))
        self.learn.xb = (xi,)

        if self._show_batch:
            self.learn.aug_x = torch.cat([xi, xj])

    def before_epoch(self):

        if self.epoch == self.n_epoch//2 and self.learn.tune_path!=None: #save e.g. after the 49th epoch
            print(f'we are saving before epoch {self.epoch}')
            self.learn.tune_path = self.learn.tune_path +f'_epochs={self.n_epoch//2}'
            torch.save(self.learn.model.state_dict(), self.learn.tune_path)

        return


    def lf(self, pred, *yb):        
        loss=self.criterion(pred,self.y)
        return loss

    @torch.no_grad()
    def show(self, n=1):
        if self._show_batch==False:
            print('Need to set show_batch=True')
            return
        bs = self.learn.aug_x.size(0)//2
        x1,x2  = self.learn.aug_x[:bs], self.learn.aug_x[bs:]
        idxs = np.random.choice(range(bs),n,False)
        x1 = self.aug1.decode(x1[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False
        x2 = self.aug2.decode(x2[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False
        images = []
        for i in range(n): images += [x1[i],x2[i]]
        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)

# %% ../nbs/cancer_maintrain.ipynb 34
class main_train:
    """Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. 
    Outputs metrics on validation data"""

    def __init__(self,
                 dls_train, #used for training BT (if pretrain=True)
                 dls_tune , #used for tuning
                 dls_valid, #used to compute metrics / evaluate results. 
                 xval, #currently `predict_model` below assumes this is entire validation / test data
                 yval,
                 aug_pipelines, #the aug pipeline for self-supervised learning
                 aug_pipelines_tune, #the aug pipeline for supervised learning
                 aug_pipelines_test, #test (or valid) time augmentations 
                 initial_weights, #Which initial weights to use
                 pretrain, #Has the model been pretrained?
                 num_epochs, #number of BT fit epochs
                 numfit, #number of tune_fit epochs
                 freeze_num_epochs, #How many epochs to freeze body for when training BT
                 freeze_numfit, #How many epochs to freeze body for when fine tuning
                 ps=8192, #projection size
                 n_in=3, #color channels
                 indim=2048, #dimension output of encoder (2048 for resnet50)
                 outdim=9, #number of classes
                 lr_max=None,#maximum learning rate used in `fit_one_cycle` (training encoder)
                 lmb=None, #generally in {1/8192, 0.5*5e-3, 5e-3} at the moment
                 print_report=False, #F1 metrics etc
                 print_plot=False, #ROC curve
                 tune_model_path=None, #base path to save: we append f'_epochs={self.numfit}' or f'_epochs={self.numfit//2}' to it
                 ):
        store_attr()
        self.vocab = self.dls_valid.vocab
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

    @staticmethod
    def fit(learn,fit_type,epochs,freeze_epochs,initial_weights,pretrain,lr_max=None):
        """We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different
        to fine_tune:bt_pretrain"""
        
        #Ok, just use `fit` for everything: keep it simple. Only wrinkle is we freeze
        #body when fine tuning IF model has been pretrained

        if fit_type == 'encoder_fine_tune': #i.e. barlow twins
            
            print(f'about to fit one cycle with lr_max: {lr_max}')
            learn.fit_one_cycle(epochs,lr_max)

        elif fit_type == 'fine_tune':

            if pretrain == False and initial_weights not in ['bt_pretrain','supervised_pretrain']: #this is equivalent to starting with all weights random:                                                                                                   #so there is no need to freeze
                learn.fit(epochs)

            else:
                learn.freeze()
                print('froze body')
        
                tune_path = learn.tune_path
                learn.tune_path=None # no saving; we don't "count" preparing the head

                learn.fit(freeze_epochs)
                learn.tune_path = tune_path
                learn.unfreeze()
                print('unfroze body')
                learn.fit(epochs)
    
        else: raise Exception('Fit policy not of expected form')
        

    def train_encoder(self):
        "create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy"

        try: #get existing encoder and plonk on new projector
            encoder = self.encoder
            encoder.cpu()
            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)
            bt_model.cuda()

        except AttributeError: #otherwise, create
            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)

        if self.pretrain: #train encoder according to fit policy

            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=self.lmb,print_augs=False)])
            main_train.fit(learn,fit_type='encoder_fine_tune',
                           epochs=self.num_epochs,freeze_epochs=self.freeze_num_epochs,
                           initial_weights=self.initial_weights,
                           pretrain=self.pretrain,
                           lr_max=self.lr_max
                          )
            
        self.encoder = bt_model.encoder
        self.bt_model = bt_model

    def fine_tune(self):
        "fine tune in supervised fashion, according to tune_fit_policy, and get metrics"

        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)

        try: 
            encoder = self.encoder
        
        except AttributeError:
            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)

        model = LM(self.encoder,numout=len(self.vocab))


        learn = Learner(self.dls_tune,model,splitter=my_splitter,
                        cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in,tune_path=self.tune_model_path)],wd=0.0
                        
                        )
        
        #debugging 
        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)

        main_train.fit(learn,fit_type='fine_tune',
                       epochs=self.numfit,freeze_epochs=self.freeze_numfit,
                       initial_weights=self.initial_weights,
                       pretrain=self.pretrain
                      ) #fine tuning (don't confuse this with fit policy!)
        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=10)
        #metrics dict will have f1 score, auc etc etc
        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)
        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)
        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval
  

        if self.tune_model_path != None:
            metrics[f'{self.numfit//2}_classif_model_path'] = self.tune_model_path +f'_epochs={self.numfit//2}'
    
            test_eq(self.tune_model_path +f'_epochs={self.numfit//2}',learn.tune_path) #check that paths match

            torch.save(model.state_dict(), self.tune_model_path + f'_epochs={self.numfit}')
            metrics[f'{self.numfit}_classif_model_path'] = self.tune_model_path + f'_epochs={self.numfit}'
        return metrics #

    def __call__(self):

        self.train_encoder() #train (or extract) the encoder
        metrics = self.fine_tune()
        
        return metrics



# %% ../nbs/cancer_maintrain.ipynb 35
@torch.no_grad()
def predict_model(xval,yval,model,aug_pipelines_test,numavg=3,criterion = CrossEntropyLossFlat(),deterministic=False):
    "Note that this assumes xval is entire validation set. If it doesn't fit in memory, can't use this guy"
    
    model.eval()

    N=xval.shape[0]
    
    if not deterministic:

        probs=0
        for _ in range(numavg):

            probs += torch.softmax(model(aug_pipelines_test[0](xval)),dim=1) #test time augmentation. This also gets around issue of randomness in the dataloader in each session...

        probs *= 1/numavg
        
    else:
        probs = torch.softmax(model(xval),dim=1)

    
    ypred = cast(torch.argmax(probs, dim=1),TensorCategory)

    correct = (ypred == yval)#.type(torch.FloatTensor)

    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)
    num_correct = correct.sum()
    accuracy = num_correct/N

    #val_loss = criterion(scores,yval)
    
    return probs,ypred,accuracy.item()#,val_loss.item()

# %% ../nbs/cancer_maintrain.ipynb 36
@torch.no_grad()
def predict_whole_model(dls_test,model,aug_pipelines_test,numavg=3,criterion = CrossEntropyLossFlat(),deterministic=False):
    "Note that this assumes xval is entire validation set. If it doesn't fit in memory, can't use this guy"
    
    model.eval()
    Acc=0
    s=0
    for i,(xval,yval) in enumerate(dls_test.train):
        
        if i == 0: #initialisation
            probs,ypred,acc = predict_model(xval,yval,model,aug_pipelines_test,numavg,
                                            criterion,deterministic
                                           )
            
            y=yval

        else:
            
            _probs,_ypred,acc = predict_model(xval,yval,model,aug_pipelines_test,
                                              numavg,criterion,deterministic
                                             )
            
            probs = torch.cat((probs,_probs))
            ypred = torch.cat((ypred,_ypred))
            y = torch.cat((y,yval))


        Acc +=acc
        
                
    Acc *= 1/(i+1)
    
    
    return y,probs,ypred,Acc
            
